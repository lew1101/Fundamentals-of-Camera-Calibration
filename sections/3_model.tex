\section{Constructing the Pinhole Camera Model}

At its core, the pinhole camera model is a mathematical model which describes how points in 3D space are projected onto a 2D plane. It defines the relationship between the 3D world, the pinhole camera, and the resulting 2D image.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{diagrams/imaging_model}
    \caption{Pinhole camera model.}
\end{figure}

\subsection{Nomenclature}
\begin{itemize}[leftmargin=!, itemindent=-4ex]
    \item $\boldsymbol{f}$ -- the focal length of the camera.
    \item $\boldsymbol{p_w}$ -- the position vector of the point $\mathtt{X}$ in $\mathcal{W}$.
    \item $\boldsymbol{p_c}$ -- the position vector of the point $\mathtt{X}$ in $\mathcal{C}$.
    \item $\boldsymbol{p_i}$ -- the position vector of the projection $\mathtt{X_i}$ on the image plane relative to the principal point, which is where the optical axis intersects the image plane.
    \item $\boldsymbol{c_w}$ -- the position vector of the pinhole in $\mathcal{W}$.
\end{itemize}

\subsection{Coordinate Frames}
To construct the pinhole camera model, we will need to define 4 different coordinate systems:
\begin{itemize}[leftmargin=!, itemindent=-4ex]
    \item\textbf{World Coordinate Frame ($\boldsymbol{\mathcal{W}}$)} -- Represents the 3D space of the scene being photographed, with respect to an origin which may be arbitrary and depends on the conventions chosen. Objects that are in the scene are defined with respect to this coordinate frame.
    \item\textbf{Camera Coordinate Frame ($\boldsymbol{\mathcal{C}}$)} -- Represents the 3D space of the scene with the pinhole (aperture) of the camera defined as the origin.
    \item\textbf{Image Coordinate Frame ($\boldsymbol{\Pi}$)} -- 2D plane representing the image sensor plane of the camera. The origin is the principle point of the image sensor, where the optical axis intersects the image plane.
    \item\textbf{Pixel Coordinate Frame} -- 2D plane representing the position of pixels on the image sensor. It is the discrete version of the image coordinate frame. The unit of measure is pixels.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{diagrams/coord_conversions}
    \caption{This workflow highlights the coordinate transformations that need to occur to project a point in the scene onto the image plane.}
\end{figure}

\subsection{Intrinsic Parameters} \label{sec:intrinsics}

Intrinsic parameters describe the internal characteristics of the camera. In other words, it dictates how in the 3D space are projected onto the image plane, i.e. the relationship between the position of $\mathtt{X}$ to its projection on the image plane.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{diagrams/perspective_projection}
    \caption{Perspective projection of the point $\mathtt{X}$ onto the image plane $\Pi$.}
\end{figure}
When a straight line is drawn from $\mathtt{X}$ to its projection $\mathtt{X_i}$ through the aperture, it intersects the optical axis. Deconstructing this intersection in the $x$ and $y$ direction, pairs of similar triangles are formed, which relates $x_i$ to $x_c$ and $y_i$ to $y_c$.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{diagrams/similar_triangles}
    \caption{Similar triangles formed by perspective projection, which relate $x_i$ to $x_c$ and $y_i$ to $y_c$.} \label{fig:similar_triangles}
\end{figure}
\begin{subequations}
    \begin{gather}
        \frac{x_i}{f} = \frac{x_c}{z_c} \quad \Longrightarrow \quad x_i = f \frac{x_c}{z_c} \label{subeq:xi_result}\\
        \frac{y_i}{f} = \frac{y_c}{z_c} \quad \Longrightarrow \quad y_i = f \frac{y_c}{z_c} \label{subeq:yi_result}
    \end{gather}
\end{subequations}
Then, we need to convert the coordinates of the image projection $[x_i, y_i]^\T$ to actual pixel position of the point on the image $[u, v]^\T$. Pixel coordinates are measured in pixels, from the left-hand corner of the image. This is the convention that is typically followed in computer graphics. As such, there will be an offset in pixels, $c_x$ and $c_y$, which represents the optical center of the image (i.e. the point at which the optical axis intersects the image plane). The relationship between $[x_i, y_i]^\T$ and $[u, v]^\T$ is proportional, but they scale at different rates, as $[x_i, y_i]^\T$ can be measured using any unit measurement, and can have negative and decimal values. On the other hand, $[u,v]^\T$ are measured in discrete pixel values, and since pixels are different sizes depending on the camera used, we define scaling factors, $m_x$ and $m_y$, which represent the pixel density of the image sensor in the $x$ and $y$ axes of the image sensor plane respectively.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{diagrams/sensor_grid}
    \caption{Conversion from image plane coordinates to sensor grid coordinates}
\end{figure}
Putting all the ideas above together, we can construct a set of linear parametric equations relating the pixel coordinates to their image coordinates thus:
\begin{align*}
    u = m_x x_i + c_x \\
    v = m_y y_i + c_y
\end{align*}
where $u, v \in \Integer^+_{\,*}$. Replacing $x_i$ and $y_i$ for the result we obtained from equations \ref{subeq:xi_result} and \ref{subeq:yi_result}, we get:
\begin{align*}
    u = m_x f \frac{x_c}{z_c} + c_x \\
    v = m_y f \frac{y_c}{z_c} + c_y
\end{align*}
This gives us a direct relationship between camera coordinates and their corresponding pixel coordinates. Since $m_x$, $m_y$, and $f$ are all unknowns, we can combine the products $m_x f$ and $m_y f$ into to $f_x$ and $f_y$ respectively. Under this new scheme, we define $f_x$ and $f_y$ as the horizontal and vertical focal lengths of camera.
\begin{subequations}
    \begin{gather*}
        u = f_x \frac{x_c}{z_c} + c_x \\
        v = f_y \frac{y_c}{z_c} + c_y
    \end{gather*}
\end{subequations}
Multiply both sides of the equations by $z_c$.
\begin{subequations}
    \begin{gather*}
        z_c u = f_x x_c + z_c c_x \\
        z_c v = f_y y_c + z_c c_y
    \end{gather*}
\end{subequations}
Doing so allows us to express the relationship as a matrix transformation using \textbf{homogeneous coordinates}\footnote{See Appendix \ref{sec:homogeneous}.}, by letting $\widetilde{w} = z_c$.
\begin{equation} \label{eq:cali}
    \begin{bmatrix}
        z_c u \\ z_c v \\ z_c
    \end{bmatrix}
    =
    a
    \begin{bmatrix}
        f_x x_c + z_c c_x \\ f_y y_c + z_c c_y \\ z_c
    \end{bmatrix}
    =
    a
    \underbrace{
        \begin{bmatrix}
            f_x & 0   & c_x \\
            0   & f_y & c_y \\
            0   & 0   & 1
        \end{bmatrix}
    }_{\mathlarger{K}}
    \begin{bmatrix}
        x_c \\ y_c \\ z_c
    \end{bmatrix}
    \eqrestriction{a \in \Real, a \neq 0}
\end{equation}

The equation includes a non-zero scale factor $a$ because a property of homogeneous coordinates is that multiplying the homogeneous coordinate of a point by a non-zero scalar still represents the same point. Equation \ref{eq:cali} can be represented more simply thus:
\begin{gather}
    \widetilde{p}_i = aK\, p_c \label{eq:pi} \eqrestriction{a \in \Real, a \neq 0}
\end{gather}
$K$ is known as the \textbf{calibration matrix}. It is a matrix transformation which maps a point represented in the camera coordinate frame to the coordinates of their projection onto the sensor plane. An important property worth noting is that $K$ is an \emph{upper triangular matrix}. It is a special kind of square matrix where all of its non-zero entries are above the main diagonal. This is an important property which we will exploit when extracting the $K$ from the projection matrix in section \ref{subsec:rq}.

\subsection{Extrinsic Parameters} \label{sec:extrinsics}

Extrinsic parameters describe the orientation of the camera by establishing the relationship between the position of a point in the world coordinate frame and its coordinates in camera coordinates.

There are two possible types of movement affecting the orientation of the camera: rotation and translation. The rotation of the camera can be described using a $3 \times 3$ square matrix:
\begin{equation}
    R =
    \begin{bmatrix}
        r_{11} & r_{12} & r_{13} \\
        r_{21} & r_{22} & r_{23} \\
        r_{31} & r_{32} & r_{33}
    \end{bmatrix}
\end{equation}
\noindent where:
\begin{itemize}
    \item \textbf{Row 1}: Unit vector representing $\hat{x}_c$ after rotation.
    \item \textbf{Row 2}: Unit vector representing $\hat{y}_c$ after rotation.
    \item \textbf{Row 3}: Unit vector representing $\hat{z}_c$ after rotation.
\end{itemize}

$R$ is an orthonormal matrix, because the row and column vectors of $R$ have to be orthogonal. Orthonormality is important because it ensures that the scale of vectors do not change (meaning that determinant of $R$ has to be 1) and that the orthogonality between vectors are maintained.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{diagrams/coord_transform}
    \caption{Coordinate transformation of $\mathtt{X}$ from the world coordinate frame to the camera coordinate frame.}
    \label{fig:ext}
\end{figure}

From Figure \ref{fig:ext}, we can see that the naive approach to finding the position vector of the point in $\mathcal{C}$  is equal to the position vector of the point in $\mathcal{W}$ minus the position vector of the camera $c_w$. However, the camera can be facing in other directions, and we account for this rotation by including a rotational matrix in the equation. Thus:
\begin{align}
    p_c & = R\,(p_w-c_w) \nonumber \\
        & = R\,p_w -R\,c_w
\end{align}
Since the position of the camera $c_w$ is constant, we let $t = -R\,c_w$, where $t$ represents the translation of the camera from the origin.
\begin{equation}
    p_c = R\,p_w + t
\end{equation}
This can be equivalently written as thus:
\begin{equation*}
    \begin{bmatrix}
        x_c \\ y_c \\ z_c
    \end{bmatrix}
    =
    \begin{bmatrix}
        r_{11} & r_{12} & r_{13} \\
        r_{21} & r_{22} & r_{23} \\
        r_{31} & r_{32} & r_{33}
    \end{bmatrix}
    \begin{bmatrix}
        x_w \\ y_w \\ z_w
    \end{bmatrix}
    +
    \begin{bmatrix}
        t_x \\ t_y \\ t_z
    \end{bmatrix}
\end{equation*}
We can combine $R$ and $t$ into an augmented matrix, $[R\,|\,t\,]$, by expressing $p_w$ in homogeneous coordinates.
\begin{equation}
    \begin{bmatrix}
        x_c \\ y_c \\ z_c
    \end{bmatrix}
    =
    \underbrace{
        \begin{bmatrix}
            r_{11} & r_{12} & r_{13} & t_x \\
            r_{21} & r_{22} & r_{23} & t_y \\
            r_{31} & r_{32} & r_{33} & t_z \\
        \end{bmatrix}
    }_{\mathlarger{[R\,|\,t\,]}}
    \begin{bmatrix}
        x_w \\ y_w \\ z_w \\ 1
    \end{bmatrix}
\end{equation}
Thus, we arrive at the final equation:
\begin{equation}
    p_c=[R\,|\,t\,] \,\widetilde{p}_w \label{eq:pc}
\end{equation}

